{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#6699ff> **Strategy of Fitting Ensemble Models:**<br/> \n",
    "\n",
    "<font color=#6699ff> 1) **Data Pre-Processing**: after reading the dataframe, we first split the training/test data by (90%-10% split) due to the small size of the dataset, then standardize the numerical columns before fitting the models, and finally checking for any missing data and impute accordingly. \n",
    "\n",
    "<font color=#6699ff> 2) **Model Score Function**: for the simplicity of model summary, we will create a model scoring function encompassing the following 6 metrics <br/> \n",
    "- $R^2$ (R Squared)\n",
    "- $EVar$ (Explained Variance Score)\n",
    "- $MAE$ (Mean Absolute Error)\n",
    "- $MSE$ (Mean Squared Error)\n",
    "- $MSLE$ (Mean Squared Log Error)\n",
    "- $MEAE$ (Median Absolute Error)\n",
    "\n",
    "<font color=#6699ff> 3) **Model Fitting**: here, we will fit 9 different ensemble regressors on the training data and then predict using the test data\n",
    "- Gradient Boosting Regressor\n",
    "- Random Forest Regressor\n",
    "- Huber Regressor\n",
    "- Elastic Net\n",
    "- SVR\n",
    "- Neural Network\n",
    "- Adaboost Regressor\n",
    "- Bagging Regressor\n",
    "- Extra Trees Regressor\n",
    "\n",
    "<font color=#6699ff> 4) **Model Summary**: after fitting all the models, we will present 3 summary tables based on training score, test score and qualitative metrics for the models\n",
    "\n",
    "<font color=#6699ff> 5) **Cross Validation**: based on the summary, we will further fine-tune the parameters on the best model by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Regression Packages\n",
    "from sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,RandomForestRegressor,ExtraTreesRegressor,BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression,HuberRegressor,ElasticNet,LassoCV,RidgeCV,PassiveAggressiveRegressor,SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,mean_squared_log_error,median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Reading Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the full data set\n",
    "data = pd.read_csv('data/Final_Dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop name column\n",
    "data = data.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2 Training/Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A train/test split is constructed where 90% of the subsample is \n",
    "# the train data set and 10% the test data set.\n",
    "\n",
    "# Set train and test sizes\n",
    "train_size = 0.9\n",
    "test_size = 1-train_size\n",
    "\n",
    "# Function to return random train and test sets\n",
    "def data_splitter(df, train, validate=False, seed=9001):\n",
    "    \n",
    "    if validate:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        validate_end = int(validate * m) + train_end\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        validate = df.ix[perm[train_end:validate_end]]\n",
    "        test = df.ix[perm[validate_end:]]\n",
    "        return train, validate, test\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        test = df.ix[perm[train_end:]]\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (1278, 949)\n",
      "Test Size: (142, 949)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test dataframes from subsample\n",
    "train_df, test_df = data_splitter(data, train_size)\n",
    "\n",
    "# Return shapes of train and test dataframes\n",
    "print(\"Train Size: {}\".format(train_df.shape))\n",
    "print(\"Test Size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List all numerical columns to be used for classification\n",
    "numerical_columns = ['acousticness_mean','acousticness_std','dance_mean','dance_std',\\\n",
    "                    'energy_mean','energy_std','instrumentalness_mean','instrumentalness_std',\\\n",
    "                    'key_mean','key_std','liveness_mean','liveness_std','loudness_mean',\\\n",
    "                    'loudness_std','mode_mean','mode_std','speech_mean','speech_std',\\\n",
    "                    'tempo_mean','tempo_std','valence_mean','valence_std','followers_mean',\\\n",
    "                    'followers_std','popularity_mean','popularity_std',\\\n",
    "                    'house_acousticness_mean', 'hip hop_acousticness_std','pop_liveness_std', \\\n",
    "                     'dance_liveness_std', 'r&b_acousticness_std','rap_energy_std', 'rap_key_std',\\\n",
    "                     'acoustic_acousticness_std','acoustic_acousticness_mean', 'acoustic_energy_std',\\\n",
    "                     'acoustic_key_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The numerical columns are standardized next\n",
    "mean = train_df[numerical_columns].mean()\n",
    "std = train_df[numerical_columns].std()\n",
    "\n",
    "train_df[numerical_columns] = (train_df[numerical_columns] - mean)/std\n",
    "test_df[numerical_columns] = (test_df[numerical_columns] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4 Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acousticness_std',\n",
       " 'dance_std',\n",
       " 'energy_std',\n",
       " 'instrumentalness_std',\n",
       " 'key_std',\n",
       " 'liveness_std',\n",
       " 'loudness_std',\n",
       " 'mode_std',\n",
       " 'speech_std',\n",
       " 'tempo_std',\n",
       " 'time_std',\n",
       " 'valence_std',\n",
       " 'followers_std',\n",
       " 'popularity_std',\n",
       " 'hip hop_acousticness_std',\n",
       " 'pop_liveness_std',\n",
       " 'dance_liveness_std',\n",
       " 'r&b_acousticness_std',\n",
       " 'rap_energy_std',\n",
       " 'rap_key_std',\n",
       " 'acoustic_acousticness_std',\n",
       " 'acoustic_energy_std',\n",
       " 'acoustic_key_std',\n",
       " 'soul_acousticness_std']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Missing Columns\n",
    "null_vals = train_df.isnull().sum()\n",
    "missing_vals = null_vals[null_vals > 0].index.tolist() \n",
    "missing_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: KNN-Based Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import KNN Regression Imputer\n",
    "# from fancyimpute import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use 3 nearest rows to fill missing observations\n",
    "# train_df[missing_vals] = KNN(k=3).complete(train_df[missing_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use 3 nearest rows to fill missing observations\n",
    "# test_df[missing_vals] = KNN(k=3).complete(test_df[missing_vals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Median-Based Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median imputation of missing values\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=1)\n",
    "train_df = pd.DataFrame(imp.fit_transform(train_df), columns=data.columns)\n",
    "test_df = pd.DataFrame(imp.transform(test_df), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "train_df = train_df[train_df['Followers'] != 0]\n",
    "test_df = test_df[test_df['Followers'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final step: create y_train/x_train and y_test/x_test dataframes\n",
    "\n",
    "# Initialize the training data\n",
    "y_train = np.log(train_df['Followers'])\n",
    "x_train = train_df.drop('Followers', axis=1)\n",
    "\n",
    "# Initialize the testing data\n",
    "y_test = np.log(test_df['Followers'])\n",
    "x_test = test_df.drop('Followers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Model Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we choose 6 metrics to evaluate our models: \n",
    "- $R^2$ (R Squared) = measures how well future datasets are likely to be predicted by the model. The score ranges from negative (because the model can be arbitrarily worse) to a best possible value of 1.0. Usually, the bigger the $R^2$, the better the model. Yet we do acknowledge the tedency of over-fitting with $R^2$ as with more predictors, it will only remain constant or increase.\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n-1}(y_i-\\hat{y}_i)^2)}{\\sum_{i=0}^{n-1}(y_i-\\bar{y})^2}, n = \\text{sample size}$$\n",
    "\n",
    "\n",
    "- $EVar$ (Explained Variance Score) = measures how good the model explains the variance in the response variable. The score ranges from a minimum of 0 to a maximum of 1.0. Similar to $R^2$, the higher the score, the better the model. \n",
    "$$EVar(y, \\hat{y}) = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)}$$\n",
    "\n",
    "\n",
    "- $MAE$ (Mean Absolute Error) = computes the expected value of the absolute error or the $l1$ loss function. For all the error functions, the smaller the error, the better the model.\n",
    "$$MAE(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n-1} |y_i-\\hat{y}_i| $$\n",
    "\n",
    "\n",
    "- $MSE$ (Mean Squared Error) = computes the expected value of the squared error\n",
    "$$MSE(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n-1} (y_i-\\hat{y}_i)^2 $$\n",
    "\n",
    "\n",
    "- $MSLE$ (Mean Squared Log Error) = computes the expected value of the squared logarithmic error. This would probably be the most appropriate metric to evalute our models as we log-transformed our response variable - number of followers for the playlist. \n",
    "$$MSLE(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n-1} [\\ln(1+y_i)-\\ln(1+\\hat{y}_i)]^2$$\n",
    "\n",
    "\n",
    "- $MEAE$ (Median Absolute Error) = computes the loss function by using the median of all absolute differences between the actual values and the predicted values. This metric is robust to outliers. \n",
    "$$MEAE(y, \\hat{y}) = \\text{median}(|y_1-\\hat{y}_1|, \\cdots, |y_n-\\hat{y}_n|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define score for regression model\n",
    "def expected_score1(model, x, y):\n",
    "    R2 = 0\n",
    "    EVar = 0\n",
    "    MAE = 0\n",
    "    MSE = 0\n",
    "    MSLE = 0\n",
    "    MEAE = 0\n",
    "\n",
    "    R2 += model.score(x, y)\n",
    "    EVar += explained_variance_score(y, model.predict(x))\n",
    "    MAE += mean_absolute_error(y, model.predict(x))\n",
    "    MSE += mean_squared_error(y, model.predict(x))\n",
    "    MSLE += mean_squared_log_error(y, model.predict(x))\n",
    "    MEAE += median_absolute_error(y, model.predict(x))\n",
    "\n",
    "    return pd.Series([R2 / 100., \n",
    "                      EVar / 100., \n",
    "                      MAE / 100., \n",
    "                      MSE / 100.,\n",
    "                      MSLE / 100.,\n",
    "                      MEAE / 100.],\n",
    "                      index = ['R2', 'EVar', 'MAE', 'MSE', 'MSLE', 'MEAE'])\n",
    "\n",
    "score = lambda model, x, y: pd.Series([model.score(x, y), \n",
    "                                       explained_variance_score(y, model.predict(x)),\n",
    "                                       mean_absolute_error(y, model.predict(x)),\n",
    "                                       mean_squared_error(y, model.predict(x)),\n",
    "                                       mean_squared_log_error(y, model.predict(x)),\n",
    "                                       median_absolute_error(y, model.predict(x))], \n",
    "                                      index=['R2', 'EVar', 'MAE', 'MSE', 'MSLE', 'MEAE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1 Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to Ben Gorman, if Linear Regression were a Toyota Camry, the Gradient Boosting Regressor would easily be a UH-60 Blackhawk Helicopter\n",
    "- Gradient Boosting Regressor is an ensemble machine learning procedure that fits new models consecutively to provide a more reliable estimate of the response variable. It constructs new base-learners to be correlated with the negative gradient of the loss function \n",
    " - least square regression (ls), \n",
    " - least absolute deviation (lad), \n",
    " - huber (a combination of ls and lad), \n",
    " - quantile - which allows for quantile regression\n",
    "- The choice of the loss function allows for great flexibility in Gradient Boosting and the best error function is huber for our model based on trial and error / cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.99, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.04, loss='huber', max_depth=5,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "estgb = GradientBoostingRegressor(alpha=0.99, loss='huber', max_depth=5, learning_rate=0.04, \n",
    "                                  n_estimators=200, max_features='auto')\n",
    "estgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estgb_training = score(estgb, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.373701\n",
       "EVar    0.373745\n",
       "MAE     1.840267\n",
       "MSE     5.357701\n",
       "MSLE    0.076379\n",
       "MEAE    1.579995\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estgb_test = score(estgb, x_test, y_test)\n",
    "estgb_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=2, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=2, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "rfrg = RandomForestRegressor(n_estimators=200, max_depth=15, max_features='auto', min_samples_leaf=2, random_state=2)\n",
    "rfrg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrg_training = score(rfrg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.327955\n",
       "EVar    0.328377\n",
       "MAE     1.908780\n",
       "MSE     5.749035\n",
       "MSLE    0.085960\n",
       "MEAE    1.667047\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfrg_test = score(rfrg, x_test, y_test)\n",
    "rfrg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3 Huber Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuberRegressor(alpha=10000, epsilon=1.0, fit_intercept=True, max_iter=100,\n",
       "        tol=1e-05, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HuberRegressor\n",
    "hubrg = HuberRegressor(max_iter=100, epsilon=1.0, alpha=10000)\n",
    "hubrg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubrg_training = score(hubrg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     -10.592332\n",
       "EVar    -0.096876\n",
       "MAE      9.485701\n",
       "MSE     99.167105\n",
       "MSLE     5.374158\n",
       "MEAE     9.923143\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubrg_test = score(hubrg, x_test, y_test)\n",
    "hubrg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4 Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.05, copy_X=True, fit_intercept=True, l1_ratio=1.0,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ElasticNet\n",
    "elarg = ElasticNet(max_iter=1000, alpha=0.05, l1_ratio=1.0)\n",
    "elarg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elarg_training = score(elarg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.249145\n",
       "EVar    0.251129\n",
       "MAE     2.026195\n",
       "MSE     6.423224\n",
       "MSLE    0.098008\n",
       "MEAE    1.736543\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elarg_test = score(elarg, x_test, y_test)\n",
    "elarg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.5 SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=10.0, cache_size=200, coef0=0.0, degree=3, epsilon=2.0, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVR\n",
    "svrrg = SVR(kernel='rbf', C=10.0, epsilon=2.0)\n",
    "svrrg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svrrg_training = score(svrrg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.145395\n",
       "EVar    0.148554\n",
       "MAE     2.139065\n",
       "MSE     7.310753\n",
       "MSLE    0.110786\n",
       "MEAE    1.882777\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrrg_test = score(svrrg, x_test, y_test)\n",
    "svrrg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.6 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-06, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Network\n",
    "mlprg = MLPRegressor(alpha=0.000001)\n",
    "mlprg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlprg_training = score(mlprg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     -2.984810e+05\n",
       "EVar   -2.942779e+05\n",
       "MAE     1.953335e+02\n",
       "MSE     2.553377e+06\n",
       "MSLE    1.149829e+00\n",
       "MEAE    3.280435e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlprg_test = score(mlprg, x_test, y_test)\n",
    "mlprg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7 Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base Estimator\n",
    "estgb_small = GradientBoostingRegressor(alpha=0.95, loss='huber', max_depth=3, learning_rate=0.01, \n",
    "                                        n_estimators=200, max_features='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=GradientBoostingRegressor(alpha=0.95, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='huber', max_depth=3,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "         learning_rate=0.04, loss='exponential', n_estimators=200,\n",
       "         random_state=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaboost Regressor\n",
    "adarg = AdaBoostRegressor(base_estimator=estgb_small, loss='exponential', learning_rate=0.04, n_estimators=200)\n",
    "adarg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adarg_training = score(adarg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.252027\n",
       "EVar    0.253087\n",
       "MAE     2.055731\n",
       "MSE     6.398564\n",
       "MSLE    0.094815\n",
       "MEAE    1.923715\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarg_test = score(adarg, x_test, y_test)\n",
    "adarg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.8 Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=GradientBoostingRegressor(alpha=0.95, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='huber', max_depth=3,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Estimator\n",
    "bagrg = BaggingRegressor(base_estimator=estgb_small, n_estimators=10, max_samples=1.0, max_features=1.0)\n",
    "bagrg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagging Regressor\n",
    "bagrg_training = score(bagrg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.216010\n",
       "EVar    0.224754\n",
       "MAE     2.027904\n",
       "MSE     6.706680\n",
       "MSLE    0.102559\n",
       "MEAE    1.696994\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagrg_test = score(bagrg, x_test, y_test)\n",
    "bagrg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.9 Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=15,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "          oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra Trees Estimator\n",
    "etreerg = ExtraTreesRegressor(n_estimators=100, criterion='mse', max_depth=15, max_features='auto')\n",
    "etreerg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etreerg_training = score(etreerg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2      0.086071\n",
       "EVar    0.086101\n",
       "MAE     2.193672\n",
       "MSE     7.818249\n",
       "MSLE    0.106772\n",
       "MEAE    1.768799\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etreerg_test = score(etreerg, x_test, y_test)\n",
    "etreerg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.1 Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- Based on the training data summary, **9.Extra Trees Regressor** best explains training data, followed by **2. Random Forest Regressor**, and **1. Gradient Boosting Regressor**\n",
    "- However, we need to bear in mind that too-good a fit on the training data suggests over-fitting - that is, the model has high variance and does not generalize the trends well (because fitted too well to the noise). This is unsurprising as just like classification trees, regression trees have the tendency to over-fit\n",
    "- We could also easily eliminate **3. Huber Regressor**, **6. Neural Network** because of their terrible performance on the training data and they might not be a good fit for the Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Gradient Boosting</th>\n",
       "      <th>2. Random Forest</th>\n",
       "      <th>3. Huber</th>\n",
       "      <th>4. Elastic Net</th>\n",
       "      <th>5. SVR</th>\n",
       "      <th>6. Neural Network</th>\n",
       "      <th>7. Adaboost</th>\n",
       "      <th>8. Bagging</th>\n",
       "      <th>9. Extra Trees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.784816</td>\n",
       "      <td>0.847263</td>\n",
       "      <td>-10.140200</td>\n",
       "      <td>0.236849</td>\n",
       "      <td>0.318037</td>\n",
       "      <td>-7.874661e+05</td>\n",
       "      <td>0.335357</td>\n",
       "      <td>0.252666</td>\n",
       "      <td>0.912714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVar</th>\n",
       "      <td>0.784823</td>\n",
       "      <td>0.847285</td>\n",
       "      <td>-0.366807</td>\n",
       "      <td>0.236849</td>\n",
       "      <td>0.320108</td>\n",
       "      <td>-7.557535e+05</td>\n",
       "      <td>0.340286</td>\n",
       "      <td>0.255436</td>\n",
       "      <td>0.912714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.093995</td>\n",
       "      <td>0.935232</td>\n",
       "      <td>9.558437</td>\n",
       "      <td>2.123556</td>\n",
       "      <td>2.012630</td>\n",
       "      <td>5.439287e+02</td>\n",
       "      <td>2.065094</td>\n",
       "      <td>2.110733</td>\n",
       "      <td>0.626782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>1.967414</td>\n",
       "      <td>1.396467</td>\n",
       "      <td>101.854169</td>\n",
       "      <td>6.977440</td>\n",
       "      <td>6.235147</td>\n",
       "      <td>7.199764e+06</td>\n",
       "      <td>6.076788</td>\n",
       "      <td>6.832826</td>\n",
       "      <td>0.798053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>5.343244</td>\n",
       "      <td>0.094873</td>\n",
       "      <td>0.089896</td>\n",
       "      <td>2.523113e+00</td>\n",
       "      <td>0.082842</td>\n",
       "      <td>0.097111</td>\n",
       "      <td>0.008469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAE</th>\n",
       "      <td>0.903242</td>\n",
       "      <td>0.796263</td>\n",
       "      <td>10.386407</td>\n",
       "      <td>1.798746</td>\n",
       "      <td>1.982707</td>\n",
       "      <td>3.022437e+00</td>\n",
       "      <td>1.930811</td>\n",
       "      <td>1.795980</td>\n",
       "      <td>0.442280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. Gradient Boosting  2. Random Forest    3. Huber  4. Elastic Net  \\\n",
       "R2                0.784816          0.847263  -10.140200        0.236849   \n",
       "EVar              0.784823          0.847285   -0.366807        0.236849   \n",
       "MAE               1.093995          0.935232    9.558437        2.123556   \n",
       "MSE               1.967414          1.396467  101.854169        6.977440   \n",
       "MSLE              0.028613          0.025686    5.343244        0.094873   \n",
       "MEAE              0.903242          0.796263   10.386407        1.798746   \n",
       "\n",
       "        5. SVR  6. Neural Network  7. Adaboost  8. Bagging  9. Extra Trees  \n",
       "R2    0.318037      -7.874661e+05     0.335357    0.252666        0.912714  \n",
       "EVar  0.320108      -7.557535e+05     0.340286    0.255436        0.912714  \n",
       "MAE   2.012630       5.439287e+02     2.065094    2.110733        0.626782  \n",
       "MSE   6.235147       7.199764e+06     6.076788    6.832826        0.798053  \n",
       "MSLE  0.089896       2.523113e+00     0.082842    0.097111        0.008469  \n",
       "MEAE  1.982707       3.022437e+00     1.930811    1.795980        0.442280  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Summary (Absolute Scores)\n",
    "training_scores = pd.DataFrame({'1. Gradient Boosting': estgb_training,\n",
    "                                '2. Random Forest': rfrg_training,\n",
    "                                '3. Huber': hubrg_training,\n",
    "                                '4. Elastic Net': elarg_training,\n",
    "                                '5. SVR': svrrg_training,\n",
    "                                '6. Neural Network': mlprg_training, \n",
    "                                '7. Adaboost': adarg_training,\n",
    "                                '8. Bagging': bagrg_training,\n",
    "                                '9. Extra Trees': etreerg_training})\n",
    "print ('Training Scores:')\n",
    "training_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores Ranking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Gradient Boosting</th>\n",
       "      <th>2. Random Forest</th>\n",
       "      <th>3. Huber</th>\n",
       "      <th>4. Elastic Net</th>\n",
       "      <th>5. SVR</th>\n",
       "      <th>6. Neural Network</th>\n",
       "      <th>7. Adaboost</th>\n",
       "      <th>8. Bagging</th>\n",
       "      <th>9. Extra Trees</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVar</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAE</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. Gradient Boosting  2. Random Forest  3. Huber  4. Elastic Net  \\\n",
       "                                                                         \n",
       "R2                     3.0               2.0       8.0             7.0   \n",
       "EVar                   3.0               2.0       8.0             7.0   \n",
       "MAE                    3.0               2.0       8.0             7.0   \n",
       "MSE                    3.0               2.0       8.0             7.0   \n",
       "MSLE                   3.0               2.0       9.0             6.0   \n",
       "MEAE                   3.0               2.0       9.0             5.0   \n",
       "\n",
       "      5. SVR  6. Neural Network  7. Adaboost  8. Bagging  9. Extra Trees  \n",
       "                                                                          \n",
       "R2       5.0                9.0          4.0         6.0             1.0  \n",
       "EVar     5.0                9.0          4.0         6.0             1.0  \n",
       "MAE      4.0                9.0          5.0         6.0             1.0  \n",
       "MSE      5.0                9.0          4.0         6.0             1.0  \n",
       "MSLE     5.0                8.0          4.0         7.0             1.0  \n",
       "MEAE     7.0                8.0          6.0         4.0             1.0  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Summary (Ranking)\n",
    "a = training_scores[0:2].rank(1, ascending=False, method='first').reset_index()\n",
    "b = training_scores[2:6].rank(1, ascending=True, method='first').reset_index()\n",
    "training_ranking = a.merge(b, how = 'outer').set_index('index')\n",
    "training_ranking.index.names = ['']\n",
    "print ('Training Scores Ranking:')\n",
    "training_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- In terms of the test data, the top 3 performers are **1. Gradient Boosting Regressor**, **2. Random Forest Regressor**, and **7. Adaboost Regressor** if we focus on $R^2$ and $MSLE$\n",
    "- The parameters in the aforementioned 3 ensemble methods could be further fine-tuned to enhance their performance in the cross-validation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Gradient Boosting</th>\n",
       "      <th>2. Random Forest</th>\n",
       "      <th>3. Huber</th>\n",
       "      <th>4. Elastic Net</th>\n",
       "      <th>5. SVR</th>\n",
       "      <th>6. Neural Network</th>\n",
       "      <th>7. Adaboost</th>\n",
       "      <th>8. Bagging</th>\n",
       "      <th>9. Extra Trees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.373701</td>\n",
       "      <td>0.327955</td>\n",
       "      <td>-10.592332</td>\n",
       "      <td>0.249145</td>\n",
       "      <td>0.145395</td>\n",
       "      <td>-2.984810e+05</td>\n",
       "      <td>0.252027</td>\n",
       "      <td>0.216010</td>\n",
       "      <td>0.086071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVar</th>\n",
       "      <td>0.373745</td>\n",
       "      <td>0.328377</td>\n",
       "      <td>-0.096876</td>\n",
       "      <td>0.251129</td>\n",
       "      <td>0.148554</td>\n",
       "      <td>-2.942779e+05</td>\n",
       "      <td>0.253087</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>0.086101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.840267</td>\n",
       "      <td>1.908780</td>\n",
       "      <td>9.485701</td>\n",
       "      <td>2.026195</td>\n",
       "      <td>2.139065</td>\n",
       "      <td>1.953335e+02</td>\n",
       "      <td>2.055731</td>\n",
       "      <td>2.027904</td>\n",
       "      <td>2.193672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>5.357701</td>\n",
       "      <td>5.749035</td>\n",
       "      <td>99.167105</td>\n",
       "      <td>6.423224</td>\n",
       "      <td>7.310753</td>\n",
       "      <td>2.553377e+06</td>\n",
       "      <td>6.398564</td>\n",
       "      <td>6.706680</td>\n",
       "      <td>7.818249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>0.076379</td>\n",
       "      <td>0.085960</td>\n",
       "      <td>5.374158</td>\n",
       "      <td>0.098008</td>\n",
       "      <td>0.110786</td>\n",
       "      <td>1.149829e+00</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>0.102559</td>\n",
       "      <td>0.106772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAE</th>\n",
       "      <td>1.579995</td>\n",
       "      <td>1.667047</td>\n",
       "      <td>9.923143</td>\n",
       "      <td>1.736543</td>\n",
       "      <td>1.882777</td>\n",
       "      <td>3.280435e+00</td>\n",
       "      <td>1.923715</td>\n",
       "      <td>1.696994</td>\n",
       "      <td>1.768799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. Gradient Boosting  2. Random Forest   3. Huber  4. Elastic Net  \\\n",
       "R2                0.373701          0.327955 -10.592332        0.249145   \n",
       "EVar              0.373745          0.328377  -0.096876        0.251129   \n",
       "MAE               1.840267          1.908780   9.485701        2.026195   \n",
       "MSE               5.357701          5.749035  99.167105        6.423224   \n",
       "MSLE              0.076379          0.085960   5.374158        0.098008   \n",
       "MEAE              1.579995          1.667047   9.923143        1.736543   \n",
       "\n",
       "        5. SVR  6. Neural Network  7. Adaboost  8. Bagging  9. Extra Trees  \n",
       "R2    0.145395      -2.984810e+05     0.252027    0.216010        0.086071  \n",
       "EVar  0.148554      -2.942779e+05     0.253087    0.224754        0.086101  \n",
       "MAE   2.139065       1.953335e+02     2.055731    2.027904        2.193672  \n",
       "MSE   7.310753       2.553377e+06     6.398564    6.706680        7.818249  \n",
       "MSLE  0.110786       1.149829e+00     0.094815    0.102559        0.106772  \n",
       "MEAE  1.882777       3.280435e+00     1.923715    1.696994        1.768799  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Summary (Absolute Scores)\n",
    "test_scores = pd.DataFrame({'1. Gradient Boosting': estgb_test,\n",
    "                            '2. Random Forest': rfrg_test,\n",
    "                            '3. Huber': hubrg_test,\n",
    "                            '4. Elastic Net': elarg_test,\n",
    "                            '5. SVR': svrrg_test,\n",
    "                            '6. Neural Network': mlprg_test, \n",
    "                            '7. Adaboost': adarg_test,\n",
    "                            '8. Bagging': bagrg_test,\n",
    "                            '9. Extra Trees': etreerg_test})\n",
    "print ('Test Scores:')\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores Ranking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Gradient Boosting</th>\n",
       "      <th>2. Random Forest</th>\n",
       "      <th>3. Huber</th>\n",
       "      <th>4. Elastic Net</th>\n",
       "      <th>5. SVR</th>\n",
       "      <th>6. Neural Network</th>\n",
       "      <th>7. Adaboost</th>\n",
       "      <th>8. Bagging</th>\n",
       "      <th>9. Extra Trees</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVar</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. Gradient Boosting  2. Random Forest  3. Huber  4. Elastic Net  \\\n",
       "                                                                         \n",
       "R2                     1.0               2.0       8.0             4.0   \n",
       "EVar                   1.0               2.0       8.0             4.0   \n",
       "MAE                    1.0               2.0       8.0             3.0   \n",
       "MSE                    1.0               2.0       8.0             4.0   \n",
       "MSLE                   1.0               2.0       9.0             4.0   \n",
       "MEAE                   1.0               2.0       9.0             4.0   \n",
       "\n",
       "      5. SVR  6. Neural Network  7. Adaboost  8. Bagging  9. Extra Trees  \n",
       "                                                                          \n",
       "R2       6.0                9.0          3.0         5.0             7.0  \n",
       "EVar     6.0                9.0          3.0         5.0             7.0  \n",
       "MAE      6.0                9.0          5.0         4.0             7.0  \n",
       "MSE      6.0                9.0          3.0         5.0             7.0  \n",
       "MSLE     7.0                8.0          3.0         5.0             6.0  \n",
       "MEAE     6.0                8.0          7.0         3.0             5.0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Summary (Ranking)\n",
    "a = test_scores[0:2].rank(1, ascending=False, method='first').reset_index()\n",
    "b = test_scores[2:6].rank(1, ascending=True, method='first').reset_index()\n",
    "test_ranking = a.merge(b, how = 'outer').set_index('index')\n",
    "test_ranking.index.names = ['']\n",
    "print ('Test Scores Ranking:')\n",
    "test_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Summary (Qualitative Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = {'max_depth': [1, 2, 3, 5, 10],\n",
    "        'learning_rate': [0.02, 0.04, 0.06, 0.08, 0.10], \n",
    "        'n_estimators': [50, 100, 200], \n",
    "        'alpha': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99], \n",
    "        'max_features': ['sqrt','auto', 'log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=GradientBoostingRegressor(), param_grid=grid, n_jobs=1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_test = score(clf, x_test, y_test)\n",
    "clf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Best Estimator Parameters\")\n",
    "print (\"loss:\", clf.best_estimator_.loss)\n",
    "print (\"max_depth: %d\" %clf.best_estimator_.max_depth)\n",
    "print (\"n_estimators: %d\" %clf.best_estimator_.n_estimators)\n",
    "print (\"learning rate: %.1f\" %clf.best_estimator_.learning_rate)\n",
    "print (\"alpha: %.1f\" %clf.best_estimator_.alpha)\n",
    "print (\"max_features:\", clf.best_estimator_.max_features)\n",
    "# print (\"min_samples_leaf:\", clf.best_estimator_.min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estgb_lin = GradientBoostingRegressor(loss='huber', max_depth=3, learning_rate=0.1, n_estimators=200)\n",
    "estgb_lin.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estgb_lin.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of Predictors: 948\n",
      "Reduced Number of Predictors: 1\n",
      "Total Explained Variance: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA that retains 90% of variance in the predictors\n",
    "pca_var = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "# Conduct PCA fit and transformation\n",
    "x_train_pca = pca_var.fit_transform(x_train)\n",
    "x_test_pca = pca_var.transform(x_test)\n",
    "\n",
    "# Show new dimensions\n",
    "print('Original Number of Predictors:', x_train.shape[1])\n",
    "print('Reduced Number of Predictors:', x_train_pca.shape[1])\n",
    "\n",
    "# Show total explained variance\n",
    "print('Total Explained Variance:', np.sum(pca_var.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='huber', max_depth=3,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estgb_lin = GradientBoostingRegressor(loss='huber', max_depth=3, learning_rate=0.1, n_estimators=200)\n",
    "estgb_lin.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30593063863110215"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estgb_lin.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
